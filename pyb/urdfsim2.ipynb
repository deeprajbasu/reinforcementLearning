{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ad1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "\n",
    "import pybullet_data\n",
    "\n",
    "\n",
    "p.connect(p.GUI)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "# First, let's make sure we start with a fresh new simulation.\n",
    "# Otherwise, we can keep adding objects by running this cell over again.\n",
    "p.resetSimulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "fe902f95-64bd-4c6e-8282-f582b8360827",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.resetSimulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385af97-66d8-425b-86e8-caca100332dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "71fb9014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.resetSimulation()\n",
    "\n",
    "# Load our simulation floor plane at the origin (0, 0, 0).\n",
    "ground = p.loadURDF('plane.urdf')\n",
    "\n",
    "# Load an R2D2 droid at the position at 0.5 meters height in the z-axis.\n",
    "r2d2 = p.loadURDF('1/bittle.urdf',  [0, 0, 2.6],globalScaling=3.5, flags=p.URDF_USE_SELF_COLLISION)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We can check the number of bodies we have in the simulation.\n",
    "p.getNumBodies()\n",
    "\n",
    "# Capture initial position\n",
    "initial_position, _ = p.getBasePositionAndOrientation(r2d2)\n",
    "initial_x, initial_y, _ = p.getLinkState(r2d2, 0)[0]\n",
    "target_x=initial_x\n",
    "target_y=10.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "6e446052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.resetBasePositionAndOrientation(r2d2,[0,0,0.6],[0,0,0,1])\n",
    "# for i in range(0,p.getNumJoints(r2d2)):\n",
    "#             p.resetJointState(bodyUniqueId=1, jointIndex=i, targetValue = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "f4e69fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2d2 unique ID: 1\n"
     ]
    }
   ],
   "source": [
    "# First let's define a class for the JointInfo.\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Joint:\n",
    "  index: int\n",
    "  name: str\n",
    "  type: int\n",
    "  gIndex: int\n",
    "  uIndex: int\n",
    "  flags: int\n",
    "  damping: float\n",
    "  friction: float\n",
    "  lowerLimit: float\n",
    "  upperLimit: float\n",
    "  maxForce: float\n",
    "  maxVelocity: float\n",
    "  linkName: str\n",
    "  axis: tuple\n",
    "  parentFramePosition: tuple\n",
    "  parentFrameOrientation: tuple\n",
    "  parentIndex: int\n",
    "\n",
    "  def __post_init__(self):\n",
    "    self.name = str(self.name, 'utf-8')\n",
    "    self.linkName = str(self.linkName, 'utf-8')\n",
    "\n",
    "# Let's analyze the R2D2 droid!\n",
    "print(f\"r2d2 unique ID: {r2d2}\")\n",
    "# for i in range(p.getNumJoints(r2d2)):\n",
    "#   joint = Joint(*p.getJointInfo(r2d2, i))\n",
    "#   print(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "7372b28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.getNumJoints(r2d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "f0dc356c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'base-frame-link': -1,\n",
       "  'battery-link': 0,\n",
       "  'cover-link': 1,\n",
       "  'left-back-shoulder-link': 2,\n",
       "  'left-back-knee-link': 3,\n",
       "  'left-front-shoulder-link': 4,\n",
       "  'left-front-knee-link': 5,\n",
       "  'mainboard_link': 6,\n",
       "  'imu_link': 7,\n",
       "  'right-back-shoulder-link': 8,\n",
       "  'right-back-knee-link': 9,\n",
       "  'right-front-shoulder-link': 10,\n",
       "  'right-front-knee-link': 11},\n",
       " dict_values([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]))"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_name_to_index = {p.getBodyInfo(r2d2)[0].decode('UTF-8'):-1,}\n",
    "        \n",
    "for id in range(p.getNumJoints(r2d2)):\n",
    "    name = p.getJointInfo(r2d2, id)[12].decode('UTF-8')\n",
    "    link_name_to_index[name] = id\n",
    "link_name_to_index, link_name_to_index.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "158b1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.changeVisualShape(r2d2,-1,rgbaColor=[0.75,0,0,1])\n",
    "# p.changeVisualShape(r2d2,0,rgbaColor=[0.5,1,0,1])\n",
    "# p.changeVisualShape(r2d2,1,rgbaColor=[1,0.45,0.2,1])\n",
    "# p.changeVisualShape(r2d2,6,rgbaColor=[0,0.3,0.8,1])\n",
    "\n",
    "p.changeVisualShape(r2d2,2,rgbaColor=[1,0.5,0.8,0.89])\n",
    "# p.changeVisualShape(r2d2,4,rgbaColor=[1,0.5,0.8,0.89])\n",
    "\n",
    "# p.changeVisualShape(r2d2,8,rgbaColor=[1,0.5,0.8,0.89])\n",
    "# p.changeVisualShape(r2d2,10,rgbaColor=[1,0.5,0.8,0.89])\n",
    "\n",
    "p.changeVisualShape(r2d2,3,rgbaColor=[1,0.5,0.5,0.89])\n",
    "# p.changeVisualShape(r2d2,5,rgbaColor=[1,0.5,0.5,0.89])\n",
    "# p.changeVisualShape(r2d2,9,rgbaColor=[1,0.5,0.5,0.89])\n",
    "# p.changeVisualShape(r2d2,11,rgbaColor=[1,0.5,0.5,0.89])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "6fa8ae43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_name_to_index.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "36e383f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0817312",
   "metadata": {},
   "source": [
    "# MODEL DEFINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "d3cdeb7c-eae0-49d4-b665-3c7247858d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "cd7fd9e5-f50f-42aa-af59-6f0f9bce06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QuadrupedNN(nn.Module):\n",
    "#     def __init__(self, input_size, output_size):\n",
    "#         super(QuadrupedNN, self).__init__()\n",
    "#         # Define the layers of the neural network\n",
    "#         self.fc1 = nn.Linear(input_size, 128)  # First hidden layer\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.fc3 = nn.Linear(64, 32)\n",
    "#         self.fc4 = nn.Linear(32, 32)\n",
    "#         self.fc5 = nn.Linear(32, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Forward pass through the network\n",
    "#         x = F.relu(self.fc1(x))  # Activation function for first layer\n",
    "#         x = F.relu(self.fc2(x))  # Activation function for second layer\n",
    "#         x = F.relu(self.fc3(x))  # Activation function for second layer\n",
    "#         x = F.relu(self.fc4(x))  # Activation function for second layer\n",
    "#         x = self.fc5(x)          # No activation for the output layer\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class QuadrupedLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "#         super(QuadrupedLSTM, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x should be of shape (batch_size, sequence_length, input_size)\n",
    "#         # Initialize hidden and cell states\n",
    "#         h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "\n",
    "#         # Forward propagate LSTM\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# class QuadrupedLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "#         super(QuadrupedLSTM, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "#         # More fully connected layers\n",
    "#         self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x should be of shape (batch_size, sequence_length, input_size)\n",
    "#         # Initialize hidden and cell states\n",
    "#         h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "\n",
    "#         # Forward propagate LSTM\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc1(out[:, -1, :])\n",
    "#         out = self.relu(out)\n",
    "#         out = self.dropout(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "class CustomSquash(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Squash the output to be in the range [-0.5, 0.5]\n",
    "        return torch.tanh(x)\n",
    "\n",
    "class QuadrupedTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, nhead=4, dropout=0.5):\n",
    "        super(QuadrupedTransformer, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.custom_activation = CustomSquash()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x should be of shape (batch_size, sequence_length, input_size)\n",
    "        x = self.embedding(x) # Transform to hidden size\n",
    "\n",
    "        # The Transformer expects input of shape (sequence_length, batch_size, hidden_size)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # Forward propagate through the Transformer\n",
    "        out = self.transformer_encoder(x)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[-1, :, :])\n",
    "        out = self.custom_activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bfcd3-2e35-41cf-955d-1ebe0f9fa9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "e48959e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Observations\n",
    "position, orientation = p.getBasePositionAndOrientation(r2d2)\n",
    "x, y, z = p.getLinkState(r2d2, 0)[0]\n",
    "roll, pitch, yaw = p.getEulerFromQuaternion(orientation)\n",
    "joint_states = p.getJointStates(r2d2, range(p.getNumJoints(r2d2)))\n",
    "joint_positions = [state[0] for state in joint_states]  # Joint positions\n",
    "joint_velocities = [0 for state in joint_states]  # Joint velocities\n",
    "contact_points = len(p.getContactPoints(bodyA=ground, bodyB=r2d2))\n",
    "\n",
    "joint_torques = [state[3] for state in joint_states]  # Joint torques\n",
    "linear_vel, angular_vel = p.getBaseVelocity(r2d2)\n",
    "\n",
    "# Combine all observations\n",
    "observations = [x, y, z, roll, pitch, yaw] + joint_positions + joint_velocities + [contact_points] +list(linear_vel)+list(angular_vel)+list(joint_torques)\n",
    "\n",
    "\n",
    "input_size = len(observations)  # This should be the length of your observation vector\n",
    "output_size = p.getNumJoints(r2d2) \n",
    "\n",
    "model = QuadrupedTransformer(input_size,4, output_size)\n",
    "# model.load_state_dict(torch.load('shared_model.pth'))\n",
    "\n",
    "# Example joint_states structure: [(position, velocity, reaction_forces, applied_effort), ...]\n",
    "foot_joint_indices = [3, 5, 9, 11,2,4,8,10]\n",
    "\n",
    "target_joint_positions = [joint_states[idx][0] for idx in foot_joint_indices]\n",
    "target_joint_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "0d4b90a8-244b-4f46-aea6-2b1c730ea5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m Get the state (position, velocity etc) for multiple joints on a body.\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p.getJointStates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "015d460d-5cdb-4341-844c-656b24a31130",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "f9e1dcb8-aab4-441c-8b10-75ecaddb01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward','priority'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity,batch_size):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.sequence_length = 32  # Define the length of each sequence\n",
    "        self.batch_size=batch_size\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sequences = []\n",
    "        for _ in range(batch_size):\n",
    "            while True:\n",
    "                start = random.randint(0, len(self.memory) - self.sequence_length)\n",
    "                sequence = list(itertools.islice(self.memory, start, start + self.sequence_length))\n",
    "                # Check if sequence crosses episode boundary\n",
    "                if not any(t.next_state is None for t in sequence):\n",
    "                    sequences.append(sequence)\n",
    "                    break\n",
    "        print(len(sequences),\"sequencec samples\",'of lenth',len(sequences[0]))\n",
    "        self.cleanup_low_reward_sequences()\n",
    "        return sequences\n",
    "\n",
    "    def calculate_sequence_reward(self, sequence):\n",
    "        return sum(transition.reward for transition in sequence)\n",
    "\n",
    "    def cleanup_low_reward_sequences(self):\n",
    "        if len(self.memory) <= self.batch_size:\n",
    "            return\n",
    "    \n",
    "        all_sequences = [list(itertools.islice(self.memory, start, start + self.sequence_length))\n",
    "                         for start in range(len(self.memory) - self.sequence_length + 1)]\n",
    "    \n",
    "        # Compute rewards for each sequence\n",
    "        sequence_rewards = [self.calculate_sequence_reward(seq) for seq in all_sequences]\n",
    "    \n",
    "        # Determine the number of sequences to remove (25% of sequences beyond batch size)\n",
    "        num_sequences_to_remove = (len(all_sequences) - self.batch_size) // 4\n",
    "    \n",
    "        # Get indices of the lowest-reward sequences\n",
    "        lowest_reward_indices = sorted(range(len(sequence_rewards)), key=lambda i: sequence_rewards[i])[:num_sequences_to_remove]\n",
    "    \n",
    "        # Remove these sequences by removing their starting transitions\n",
    "        for idx in sorted(lowest_reward_indices, reverse=True):\n",
    "            del self.memory[idx]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "batch_size=128\n",
    "replay_memory = ReplayMemory(capacity=1024,batch_size =batch_size)  # Adjust the capacity as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "592c26b3-af63-4512-ae24-2fe316ba3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.setPhysicsEngineParameter(fixedTimeStep=1/80)\n",
    "# Define the maximum velocity limit\n",
    "max_velocity_limit =5.11604775654# Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "ab13eed9-e371-4e8e-b1ae-a2a1724a16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data=[]\n",
    "reward_data = []\n",
    "jointdata=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "4f9e57cd-7235-43b3-8c62-19681d608e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the gravity to Earth's gravity.\n",
    "p.setGravity(0, 0, -9.807)\n",
    "\n",
    "for i in range(100):\n",
    "    p.stepSimulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "65928ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 1\n",
      "128 sequencec samples of lenth 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91917\\AppData\\Local\\Temp\\ipykernel_21192\\1803375906.py:251: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(current_q_values, td_target.unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training -1\n",
      "128 sequencec samples of lenth 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[724], line 241\u001b[0m\n\u001b[0;32m    235\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m model(state_batch)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# Predict next Q-values\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Average next Q-values across joints\u001b[39;00m\n\u001b[0;32m    244\u001b[0m average_next_q_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(next_q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[673], line 95\u001b[0m, in \u001b[0;36mQuadrupedTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(x)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Forward propagate through the Transformer\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Decode the hidden state of the last time step\u001b[39;00m\n\u001b[0;32m     98\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :])\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:708\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[1;32m--> 708\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:723\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 723\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)))\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#SIM\n",
    "rec=300\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "    \n",
    "def calculate_reward(position, orientation, joint_states, linear_vel, angular_vel, initial_position, initial_orientation,contact_points,walk):\n",
    "    # Constants\n",
    "    HEIGHT_TARGET = 4  # Target height for the robot\n",
    "    HEIGHT_WEIGHT =561000000231500.25 # Weight for height reward\n",
    "    ORIENTATION_WEIGHT =180100123000123000000000000.00405104687  # Weight for orientation penalty\n",
    "    POSITION_WEIGHT = 0 # Weight for position penalty\n",
    "    MOVEMENT_WEIGHT =10.45  # Weight for movement penalty\n",
    "    CONTACT_POINTS_PENALTY_WEIGHT = 100000000123000000.16548  # Weight for contact points reward/penalty\n",
    "    CONTACT_POINTS_REWARD_WEIGHT = 150000000000000000.553\n",
    "    LEG_POSITION_PENALTY_WEIGHT = 0.035130  # Adjust as needed\n",
    "    TORQUE_PENALTY_WEIGHT = 2.450\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    x, y, z = position\n",
    "    initial_x, initial_y, _ = initial_position\n",
    "\n",
    "    # Height Reward\n",
    "    height_reward = -HEIGHT_WEIGHT * abs(z - HEIGHT_TARGET)\n",
    "\n",
    "    # Orientation Penalty\n",
    "    orientation_error = np.linalg.norm(np.array(orientation) - np.array(initial_orientation))\n",
    "    orientation_penalty = -ORIENTATION_WEIGHT * orientation_error\n",
    "\n",
    "    \n",
    "    if walk :   \n",
    "        position_penalty = (1001232131400000000.115513*y)-POSITION_WEIGHT * (abs(x - target_x) + abs(y - target_y))\n",
    "    # Position Penalty\n",
    "    else : \n",
    "        position_penalty = -POSITION_WEIGHT * (abs(x - initial_x) + abs(y - initial_y))\n",
    "\n",
    "    # Movement Penalty\n",
    "    joint_velocities = [state[1] for state in joint_states]\n",
    "    movement_penalty = -MOVEMENT_WEIGHT * (np.linalg.norm(joint_velocities) + np.linalg.norm(linear_vel) + np.linalg.norm(angular_vel))\n",
    "\n",
    "\n",
    "\n",
    "    contact_reward, contact_penalty= calculate_contact_reward_penalty(joint_states)\n",
    "\n",
    "    contact_reward = CONTACT_POINTS_REWARD_WEIGHT*contact_reward\n",
    "    contact_penalty = -CONTACT_POINTS_PENALTY_WEIGHT*contact_penalty\n",
    "    \n",
    "    joint_torques = [state[3] for state in joint_states]  # Extract joint torques\n",
    "    torque_penalty = -TORQUE_PENALTY_WEIGHT * sum(abs(torque) for torque in joint_torques)\n",
    "\n",
    "\n",
    "    current_joint_positions = [joint_states[idx][0] for idx in foot_joint_indices]\n",
    "    leg_dev = [abs(i-j) for i in current_joint_positions for j in target_joint_positions ]\n",
    "    leg_position_penalty = -LEG_POSITION_PENALTY_WEIGHT*sum(leg_dev)  # Adjust as neededcurrent_joint_positions\n",
    "\n",
    "    # Total Reward\n",
    "    total_reward = height_reward + orientation_penalty + position_penalty + movement_penalty + torque_penalty + leg_position_penalty +contact_reward + contact_penalty\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def calculate_contact_reward_penalty(joint_states,leg_joint_indices=[3,9,5,11]):\n",
    "    contact_reward = 0\n",
    "    contact_penalty = 0\n",
    "\n",
    "    for joint_index in range(len(joint_states)):\n",
    "        # Check for contact with the ground\n",
    "        contacts = p.getContactPoints(bodyA=r2d2, bodyB=ground, linkIndexA=joint_index)\n",
    "\n",
    "        if contacts:\n",
    "            if joint_index in leg_joint_indices:\n",
    "                # Reward for leg joints making contact\n",
    "                contact_reward += 10\n",
    "            else:\n",
    "                # Penalty for other joints making contact\n",
    "                contact_penalty += 1\n",
    "\n",
    "    return contact_reward, contact_penalty\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "initial_orientation = orientation\n",
    "\n",
    "\n",
    "# Run the simulation for a fixed amount of steps.\n",
    "observations = []\n",
    "\n",
    "\n",
    "# Initialize training variables\n",
    "train = True  # Set this to True or False\n",
    "training_data = []  # To store (observation, action, reward)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Example optimizer\n",
    "\n",
    "walk = True\n",
    "\n",
    "t=0\n",
    "tt=64\n",
    "train_alternator = -1 \n",
    "#start sim\n",
    "episode=1\n",
    "while True :\n",
    "    t+=1\n",
    "    if t==tt:\n",
    "        p.resetBasePositionAndOrientation(r2d2, initial_position, initial_orientation)\n",
    "\n",
    "        for joint in  range(p.getNumJoints(r2d2)):\n",
    "            p.resetJointState(r2d2, joint, targetValue=0, targetVelocity=0)\n",
    "        t=0\n",
    "        episode+=1\n",
    "        # print(episode,\":episode finised\")\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    ##OBSERVATIONS\n",
    "    \n",
    "    # Observations\n",
    "    position, orientation = p.getBasePositionAndOrientation(r2d2)\n",
    "    x, y, z = p.getLinkState(r2d2, 0)[0]\n",
    "    roll, pitch, yaw = p.getEulerFromQuaternion(orientation)\n",
    "    joint_states = p.getJointStates(r2d2, range(p.getNumJoints(r2d2)))\n",
    "    joint_positions = [state[0] for state in joint_states]  # Joint positions\n",
    "    joint_velocities = [state[1] for state in joint_states]  # Joint velocities\n",
    "    contact_points = len(p.getContactPoints(bodyA=ground, bodyB=r2d2))\n",
    "    linear_vel, angular_vel = p.getBaseVelocity(r2d2)\n",
    "\n",
    "    joint_torques = [state[3] for state in joint_states]  # Joint torques\n",
    "    \n",
    "    # Combine all observations\n",
    "    observations = [x, y, z, roll, pitch, yaw] + joint_positions + joint_velocities + [contact_points] + list(linear_vel) +list(angular_vel) +list(joint_torques)\n",
    "    # print(contact_points)\n",
    "    # # Convert observations to a PyTorch tensor\n",
    "    # observations_tensor = torch.tensor(observations, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    observations_tensor = torch.tensor(observations, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    observations_tensor = observations_tensor.to(device)\n",
    "    actions = model(observations_tensor).detach().cpu().numpy()[0]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(p.getNumJoints(r2d2)):\n",
    "       \n",
    "        # Ensure actions are in the correct format and range for your specific robot\n",
    "\n",
    "        # action = np.clip(scaled_actions[i], -SC, SC)\n",
    "        # joint_action = np.clip(scaled_actions[i], -max_velocity_limit, max_velocity_limit) \n",
    "        # if i==8:\n",
    "        #     jointdata.append(joint_action)\n",
    "        # print(action)\n",
    "        p.setJointMotorControl2(bodyIndex=1,jointIndex=i, controlMode=p.VELOCITY_CONTROL, targetVelocity=max_velocity_limit*actions[i])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # next Observations\n",
    "    _, orientation = p.getBasePositionAndOrientation(r2d2)\n",
    "    position = p.getLinkState(r2d2, 0)[0]\n",
    "    roll, pitch, yaw = p.getEulerFromQuaternion(orientation)\n",
    "    joint_states = p.getJointStates(r2d2, range(p.getNumJoints(r2d2)))\n",
    "    joint_positions = [state[0] for state in joint_states]  # Joint positions\n",
    "    joint_velocities = [state[1] for state in joint_states]  # Joint velocities\n",
    "    contact_points = len(p.getContactPoints(bodyA=ground, bodyB=r2d2))\n",
    "    linear_vel, angular_vel = p.getBaseVelocity(r2d2)\n",
    "    joint_torques = [state[3] for state in joint_states]  # Joint torques\n",
    "    \n",
    "    # Combine all observations\n",
    "    next_observations = [x, y, z, roll, pitch, yaw] + joint_positions + joint_velocities + [contact_points] + list(linear_vel) +list(angular_vel) +list(joint_torques)\n",
    "\n",
    "    # # Convert observations to a PyTorch tensor\n",
    "    # next_observations_tensor = torch.tensor(next_observations, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    reward = calculate_reward(position, orientation, joint_states, linear_vel, angular_vel, initial_position, initial_orientation,contact_points,walk)\n",
    "\n",
    "    # if t % 1 == 0:\n",
    "    #         reward_data.append(contact_points)\n",
    "    # if contact_points == 4:  # Assuming 4 is the desired number of contact points\n",
    "    #     reward += contact_bonus  # Balance reward\n",
    "    # else:  # Assuming 4 is the desired number of contact points\n",
    "    #     reward -= 0.02  # Balance reward\n",
    "    \n",
    "    # Store data for training\n",
    "    if train:\n",
    "        # Convert observations and next_observations to PyTorch tensors\n",
    "        observations_tensor = torch.tensor(observations, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        next_observations_tensor = torch.tensor(next_observations, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "    \n",
    "        # Store the transition in replay memory\n",
    "    \n",
    "        # Calculate initial priority for the new experience\n",
    "        # Using absolute reward as a simple priority measure\n",
    "        initial_priority = abs(reward)\n",
    "\n",
    "        # Store the transition with priority in replay memory\n",
    "        replay_memory.push(observations_tensor, actions_tensor, next_observations_tensor, reward_tensor, initial_priority)\n",
    "\n",
    "\n",
    "    # Perform training \n",
    "\n",
    "    if train and len(replay_memory) >= batch_size and episode%88==0 :\n",
    "        episode+=1\n",
    "        train_alternator=train_alternator*-1\n",
    "        print('training',train_alternator)\n",
    "        gamma = 0.99\n",
    "        sequences = replay_memory.sample(batch_size)\n",
    "        for sequence in sequences:\n",
    "\n",
    "            \n",
    "            batch = Transition(*zip(*sequence))\n",
    "        \n",
    "            # Concatenate the batch elements into separate tensors\n",
    "            state_batch = torch.cat(batch.state).unsqueeze(1)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "            next_state_batch = torch.cat(batch.next_state).unsqueeze(1)\n",
    "        \n",
    "            # Predict current Q-values\n",
    "            # print(state_batch.shape,reward_batch.shape)\n",
    "            current_q_values = model(state_batch)\n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "            # Predict next Q-values\n",
    "            next_q_values = model(next_state_batch)\n",
    "        \n",
    "            # Average next Q-values across joints\n",
    "            average_next_q_values = torch.mean(next_q_values, dim=1)\n",
    "        \n",
    "            # Calculate TD target\n",
    "            td_target = reward_batch + gamma * average_next_q_values\n",
    "        \n",
    "            # Compute loss\n",
    "            # print(current_q_values)\n",
    "            loss = F.mse_loss(current_q_values, td_target.unsqueeze(1))\n",
    "    \n",
    "            if t % 100 == 0:\n",
    "                loss_data.append(loss.detach().cpu().item())\n",
    "    \n",
    "        \n",
    "            # Perform optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    p.stepSimulation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01585598-115a-4a7a-b775-803845ed3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "jointdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a419a6-95d3-40ad-9545-3f4a3995cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(jointdata)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08750e88-81a7-4f69-a563-a61ba5c6ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a143f",
   "metadata": {},
   "source": [
    "## Replay memory\n",
    "\n",
    "$ e_t=(s_t,a_t,r_{t+1},s_{t+1}) $\n",
    "\n",
    "This tuple contains the state of the environment , the action taken from state , the reward given to the agent at time as a result of the previous state-action pair , and the next state of the environment . This tuple indeed gives us a summary of the agent's experience at time . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec38f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8afb1",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a21202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e17532",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "# init_screen = get_screen()\n",
    "screen_height, screen_width =1,12\n",
    "\n",
    "# # Get number of actions from gym action space\n",
    "n_actions = 12\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41abd7",
   "metadata": {},
   "source": [
    "## TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "\n",
    "\n",
    "    for t in count():\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "            \n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "width = 320\n",
    "height = 300\n",
    "img_arr = p.getCameraImage(\n",
    "    width,\n",
    "    height,\n",
    "    viewMatrix=p.computeViewMatrixFromYawPitchRoll(\n",
    "        cameraTargetPosition=[0, 0, 0],\n",
    "        distance=2,\n",
    "        yaw=60,\n",
    "        pitch=-10,\n",
    "        roll=0,\n",
    "        upAxisIndex=2,\n",
    "    ),\n",
    "    projectionMatrix=p.computeProjectionMatrixFOV(\n",
    "        fov=60,\n",
    "        aspect=width/height,\n",
    "        nearVal=0.01,\n",
    "        farVal=100,\n",
    "    ),\n",
    "    shadow=True,\n",
    "    lightDirection=[1, 1, 1],\n",
    ")\n",
    "width, height, rgba, depth, mask = img_arr\n",
    "print(f\"rgba shape={rgba.shape}, dtype={rgba.dtype}\")\n",
    "display(Image.fromarray(rgba, 'RGBA'))\n",
    "print(f\"depth shape={depth.shape}, dtype={depth.dtype}, as values from 0.0 (near) to 1.0 (far)\")\n",
    "display(Image.fromarray((depth*255).astype('uint8')))\n",
    "print(f\"mask shape={mask.shape}, dtype={mask.dtype}, as unique values from 0 to N-1 entities, and -1 as None\")\n",
    "display(Image.fromarray(np.interp(mask, (-1, mask.max()), (0, 255)).astype('uint8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b455645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49549f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.changeVisualShape(r2d2,-1,rgbaColor=[1,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b572681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
