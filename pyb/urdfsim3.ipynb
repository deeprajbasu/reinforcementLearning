{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ad1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "from numpy import random\n",
    "import pybullet_data\n",
    "\n",
    "\n",
    "p.connect(p.DIRECT)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "# First, let's make sure we start with a fresh new simulation.\n",
    "# Otherwise, we can keep adding objects by running this cell over again.\n",
    "p.resetSimulation()\n",
    "\n",
    "p.setPhysicsEngineParameter(fixedTimeStep=1/100)\n",
    "# Set the gravity to Earth's gravity.\n",
    "p.setGravity(0, 0, -9.807)\n",
    "\n",
    "# Define the maximum velocity limit\n",
    "max_velocity_limit =2.11604775654# Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914f6e91-dde2-4dfa-a2f9-e0abe39313e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # p.changeVisualShape(r2d2,-1,rgbaColor=[0.75,0,0,1])\n",
    "# # p.changeVisualShape(r2d2,0,rgbaColor=[0.5,1,0,1])\n",
    "# # p.changeVisualShape(r2d2,1,rgbaColor=[1,0.45,0.2,1])\n",
    "# # p.changeVisualShape(r2d2,6,rgbaColor=[0,0.3,0.8,1])\n",
    "\n",
    "# p.changeVisualShape(r2d2,2,rgbaColor=[1,0.5,0.8,0.89])\n",
    "# # p.changeVisualShape(r2d2,4,rgbaColor=[1,0.5,0.8,0.89])\n",
    "\n",
    "# # p.changeVisualShape(r2d2,8,rgbaColor=[1,0.5,0.8,0.89])\n",
    "# # p.changeVisualShape(r2d2,10,rgbaColor=[1,0.5,0.8,0.89])\n",
    "\n",
    "# p.changeVisualShape(r2d2,3,rgbaColor=[1,0.5,0.5,0.89])\n",
    "# # p.changeVisualShape(r2d2,5,rgbaColor=[1,0.5,0.5,0.89])\n",
    "# # p.changeVisualShape(r2d2,9,rgbaColor=[1,0.5,0.5,0.89])\n",
    "# # p.changeVisualShape(r2d2,11,rgbaColor=[1,0.5,0.5,0.89])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc228f0-7434-49fa-bcd1-553e90b43509",
   "metadata": {},
   "source": [
    "# replay memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c3fe8f-58b0-4cb8-b69a-58c2e4f99084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward','priority'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity,batch_size):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.sequence_length = 128  # Define the length of each sequence\n",
    "        self.batch_size=batch_size\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sequences = []\n",
    "        for _ in range(batch_size):\n",
    "            while True:\n",
    "                start = random.randint(0, len(self.memory) - self.sequence_length)\n",
    "                sequence = list(itertools.islice(self.memory, start, start + self.sequence_length))\n",
    "                # Check if sequence crosses episode boundary\n",
    "                if not any(t.next_state is None for t in sequence):\n",
    "                    sequences.append(sequence)\n",
    "                    break\n",
    "        print(len(sequences),\"sequencec samples\",'of lenth',len(sequences[0]))\n",
    "        self.cleanup_low_reward_sequences()\n",
    "        return sequences\n",
    "\n",
    "    def calculate_sequence_reward(self, sequence):\n",
    "        return sum(transition.reward for transition in sequence)\n",
    "\n",
    "    def cleanup_low_reward_sequences(self):\n",
    "        if len(self.memory) <= self.batch_size:\n",
    "            return\n",
    "    \n",
    "        all_sequences = [list(itertools.islice(self.memory, start, start + self.sequence_length))\n",
    "                         for start in range(len(self.memory) - self.sequence_length + 1)]\n",
    "    \n",
    "        # Compute rewards for each sequence\n",
    "        sequence_rewards = [self.calculate_sequence_reward(seq) for seq in all_sequences]\n",
    "    \n",
    "        # Determine the number of sequences to remove (25% of sequences beyond batch size)\n",
    "        num_sequences_to_remove = (len(all_sequences) - self.batch_size) // 4\n",
    "    \n",
    "        # Get indices of the lowest-reward sequences\n",
    "        lowest_reward_indices = sorted(range(len(sequence_rewards)), key=lambda i: sequence_rewards[i])[:num_sequences_to_remove]\n",
    "    \n",
    "        # Remove these sequences by removing their starting transitions\n",
    "        for idx in sorted(lowest_reward_indices, reverse=True):\n",
    "            del self.memory[idx]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "batch_size=32\n",
    "replay_memory = ReplayMemory(capacity=4096,batch_size =batch_size)  # Adjust the capacity as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0817312",
   "metadata": {},
   "source": [
    "# MODEL DEFINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3cdeb7c-eae0-49d4-b665-3c7247858d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7fd9e5-f50f-42aa-af59-6f0f9bce06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QuadrupedNN(nn.Module):\n",
    "#     def __init__(self, input_size, output_size):\n",
    "#         super(QuadrupedNN, self).__init__()\n",
    "#         # Define the layers of the neural network\n",
    "#         self.fc1 = nn.Linear(input_size, 128)  # First hidden layer\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.fc3 = nn.Linear(64, 32)\n",
    "#         self.fc4 = nn.Linear(32, 32)\n",
    "#         self.fc5 = nn.Linear(32, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Forward pass through the network\n",
    "#         x = F.relu(self.fc1(x))  # Activation function for first layer\n",
    "#         x = F.relu(self.fc2(x))  # Activation function for second layer\n",
    "#         x = F.relu(self.fc3(x))  # Activation function for second layer\n",
    "#         x = F.relu(self.fc4(x))  # Activation function for second layer\n",
    "#         x = self.fc5(x)          # No activation for the output layer\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class QuadrupedLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "#         super(QuadrupedLSTM, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x should be of shape (batch_size, sequence_length, input_size)\n",
    "#         # Initialize hidden and cell states\n",
    "#         h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "\n",
    "#         # Forward propagate LSTM\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# class QuadrupedLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "#         super(QuadrupedLSTM, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "#         # More fully connected layers\n",
    "#         self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x should be of shape (batch_size, sequence_length, input_size)\n",
    "#         # Initialize hidden and cell states\n",
    "#         h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "\n",
    "#         # Forward propagate LSTM\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc1(out[:, -1, :])\n",
    "#         out = self.relu(out)\n",
    "#         out = self.dropout(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "class CustomSquash(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Squash the output to be in the range [-0.5, 0.5]\n",
    "        return torch.tanh(x)\n",
    "\n",
    "class QuadrupedTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, nhead=4, dropout=0.5):\n",
    "        super(QuadrupedTransformer, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.custom_activation = CustomSquash()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x should be of shape (batch_size, sequence_length, input_size)\n",
    "        x = self.embedding(x) # Transform to hidden size\n",
    "\n",
    "        # The Transformer expects input of shape (sequence_length, batch_size, hidden_size)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # Forward propagate through the Transformer\n",
    "        out = self.transformer_encoder(x)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[-1, :, :])\n",
    "        out = self.custom_activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c479d-99c6-4efc-ab47-1284691c867d",
   "metadata": {},
   "source": [
    "# Model init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48959e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe902f95-64bd-4c6e-8282-f582b8360827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.resetSimulation()\n",
    "\n",
    "# Load our simulation floor plane at the origin (0, 0, 0).\n",
    "ground = p.loadURDF('plane.urdf')\n",
    "\n",
    "    \n",
    "    \n",
    "# We can check the number of bodies we have in the simulation.\n",
    "p.getNumBodies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87180f70-ecd1-4de2-9d96-9bc648f66c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "def calculate_reward(position, orientation, joint_states, linear_vel, angular_vel, initial_position, initial_orientation,contact_points):\n",
    "    # Constants\n",
    "    HEIGHT_TARGET = 30  # Target height for the robot\n",
    "    HEIGHT_WEIGHT = 0 # Weight for height reward\n",
    "    ORIENTATION_WEIGHT =66.405104687  # Weight for orientation penalty\n",
    "    POSITION_WEIGHT = 0.3320 # Weight for position penalty\n",
    "    MOVEMENT_WEIGHT =31.45  # Weight for movement penalty\n",
    "    CONTACT_POINTS_PENALTY_WEIGHT = 150.16548  # Weight for contact points reward/penalty\n",
    "    CONTACT_POINTS_REWARD_WEIGHT = 168.553\n",
    "    LEG_POSITION_PENALTY_WEIGHT = 64.35130  # Adjust as needed\n",
    "    TORQUE_PENALTY_WEIGHT = 54.450\n",
    "\n",
    "    x, y, z = position\n",
    "    initial_x, initial_y, _ = initial_position\n",
    "\n",
    "    # Height Reward\n",
    "    height_reward = -HEIGHT_WEIGHT * abs(z - HEIGHT_TARGET)\n",
    "\n",
    "    # Orientation Penalty\n",
    "    orientation_error = np.linalg.norm(np.array(orientation) - np.array(initial_orientation))\n",
    "    orientation_penalty = -ORIENTATION_WEIGHT * orientation_error\n",
    "\n",
    "    # Position Penalty\n",
    "    position_penalty = -POSITION_WEIGHT * (abs(x - initial_x) + abs(y - initial_y))\n",
    "\n",
    "    # Movement Penalty\n",
    "    joint_velocities = [state[1] for state in joint_states]\n",
    "    movement_penalty = -MOVEMENT_WEIGHT * (np.linalg.norm(joint_velocities) + np.linalg.norm(linear_vel) + np.linalg.norm(angular_vel))\n",
    "\n",
    "\n",
    "\n",
    "    contact_reward, contact_penalty= calculate_contact_reward_penalty(joint_states)\n",
    "\n",
    "    contact_reward = CONTACT_POINTS_REWARD_WEIGHT*contact_reward\n",
    "    contact_penalty = -CONTACT_POINTS_PENALTY_WEIGHT*contact_penalty\n",
    "    \n",
    "    joint_torques = [state[3] for state in joint_states]  # Extract joint torques\n",
    "    torque_penalty = -TORQUE_PENALTY_WEIGHT * sum(abs(torque) for torque in joint_torques)\n",
    "\n",
    "\n",
    "    current_joint_positions = [joint_states[idx][0] for idx in foot_joint_indices]\n",
    "    leg_dev = [abs(i-j) for i in current_joint_positions for j in target_joint_positions ]\n",
    "    leg_position_penalty = -LEG_POSITION_PENALTY_WEIGHT*sum(leg_dev)  # Adjust as neededcurrent_joint_positions\n",
    "\n",
    "    # Total Reward\n",
    "    total_reward = height_reward + orientation_penalty + position_penalty + movement_penalty + torque_penalty + leg_position_penalty +contact_reward + contact_penalty\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def calculate_contact_reward_penalty(joint_states,leg_joint_indices=[3,9,5,11]):\n",
    "    contact_reward = 0\n",
    "    contact_penalty = 0\n",
    "\n",
    "    for joint_index in range(len(joint_states)):\n",
    "        # Check for contact with the ground\n",
    "        contacts = p.getContactPoints(bodyA=r2d2, bodyB=ground, linkIndexA=joint_index)\n",
    "\n",
    "        if contacts:\n",
    "            if joint_index in leg_joint_indices:\n",
    "                # Reward for leg joints making contact\n",
    "                contact_reward += 1\n",
    "            else:\n",
    "                # Penalty for other joints making contact\n",
    "                contact_penalty += 1\n",
    "\n",
    "    return contact_reward, contact_penalty\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385af97-66d8-425b-86e8-caca100332dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(filename='trakkkkining.log', level=logging.DEBUG,\n",
    "                        format='%(asctime)s %(levelname)s %(processName)s: %(message)s')\n",
    "\n",
    "\n",
    "\n",
    "def train_robot(process_id, model_queue, best_model_event):\n",
    "\n",
    "    setup_logging()\n",
    "    logging.debug(f\"Starting process {process_id}\")\n",
    "\n",
    "    replay_memory = ReplayMemory(capacity=4096,batch_size =batch_size)  # Adjust the capacity as needed\n",
    "\n",
    "\n",
    "    \n",
    "    # Initialize PyBullet, model, and other necessary components\n",
    "\n",
    "    p.resetSimulation()\n",
    "    \n",
    "    # Load an R2D2 droid at the position at 0.5 meters height in the z-axis.\n",
    "    r2d2 = p.loadURDF('1/bittle.urdf',  [5*process_id, 0, 2.6],globalScaling=3.5, flags=p.URDF_USE_SELF_COLLISION)\n",
    "\n",
    "        \n",
    "    for i in range(4000):\n",
    "        p.stepSimulation()\n",
    "\n",
    "\n",
    "    \n",
    "    # Capture initial position\n",
    "    initial_position, _ = p.getBasePositionAndOrientation(r2d2)\n",
    "    initial_x, initial_y, _ = initial_position\n",
    "\n",
    "\n",
    "\n",
    "    # Observations\n",
    "    position, orientation = p.getBasePositionAndOrientation(r2d2)\n",
    "    x, y, z = position\n",
    "    roll, pitch, yaw = p.getEulerFromQuaternion(orientation)\n",
    "    joint_states = p.getJointStates(r2d2, range(p.getNumJoints(r2d2)))\n",
    "    joint_positions = [state[0] for state in joint_states]  # Joint positions\n",
    "    joint_velocities = [0 for state in joint_states]  # Joint velocities\n",
    "    contact_points = len(p.getContactPoints(bodyA=ground, bodyB=r2d2))\n",
    "    \n",
    "    joint_torques = [state[3] for state in joint_states]  # Joint torques\n",
    "    linear_vel, angular_vel = p.getBaseVelocity(r2d2)\n",
    "    \n",
    "    # Combine all observations\n",
    "    observations = [x, y, z, roll, pitch, yaw] + joint_positions + joint_velocities + [contact_points] +list(linear_vel)+list(angular_vel)+list(joint_torques)\n",
    "    \n",
    "    \n",
    "    input_size = len(observations)  # This should be the length of your observation vector\n",
    "    output_size = p.getNumJoints(r2d2) \n",
    "    \n",
    "    \n",
    "    if best_model_event.is_set():\n",
    "        model = model_queue.get()\n",
    "        best_model_event.clear()\n",
    "    else : \n",
    "        #model init\n",
    "        model = QuadrupedTransformer(input_size,4, output_size)\n",
    "        model = model.to(device)\n",
    "    \n",
    "    \n",
    "    # Example joint_states structure: [(position, velocity, reaction_forces, applied_effort), ...]\n",
    "    foot_joint_indices = [3, 5, 9, 11,2,4,8,10]\n",
    "    \n",
    "    target_joint_positions = [joint_states[idx][0] for idx in foot_joint_indices]\n",
    "    target_joint_positions\n",
    "\n",
    "\n",
    "\n",
    "    initial_orientation = orientation\n",
    "    \n",
    "    \n",
    "    # Run the simulation for a fixed amount of steps.\n",
    "    observations = []\n",
    "    \n",
    "    \n",
    "    # Initialize training variables\n",
    "    train = True  # Set this to True or False\n",
    "    training_data = []  # To store (observation, action, reward)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Example optimizer\n",
    "    \n",
    "    t=0\n",
    "    tt=128\n",
    "    train_alternator = -1 \n",
    "    #start sim\n",
    "    episode=1\n",
    "    trains = 0\n",
    "    while trains<15 :\n",
    "        t+=1\n",
    "        if t==tt:\n",
    "            p.resetBasePositionAndOrientation(r2d2, initial_position, initial_orientation)\n",
    "    \n",
    "            for joint in  range(p.getNumJoints(r2d2)):\n",
    "                p.resetJointState(r2d2, joint, targetValue=0, targetVelocity=0)\n",
    "            t=0\n",
    "            episode+=1\n",
    "            # print(episode,\":episode finised\")\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    \n",
    "        ##OBSERVATIONS\n",
    "        \n",
    "        # Observations\n",
    "        position, orientation = p.getBasePositionAndOrientation(r2d2)\n",
    "        x, y, z = position\n",
    "        roll, pitch, yaw = p.getEulerFromQuaternion(orientation)\n",
    "        joint_states = p.getJointStates(r2d2, range(p.getNumJoints(r2d2)))\n",
    "        joint_positions = [state[0] for state in joint_states]  # Joint positions\n",
    "        joint_velocities = [state[1] for state in joint_states]  # Joint velocities\n",
    "        contact_points = len(p.getContactPoints(bodyA=ground, bodyB=r2d2))\n",
    "        linear_vel, angular_vel = p.getBaseVelocity(r2d2)\n",
    "    \n",
    "        joint_torques = [state[3] for state in joint_states]  # Joint torques\n",
    "        \n",
    "        # Combine all observations\n",
    "        observations = [x, y, z, roll, pitch, yaw] + joint_positions + joint_velocities + [contact_points] + list(linear_vel) +list(angular_vel) +list(joint_torques)\n",
    "        # print(contact_points)\n",
    "        # # Convert observations to a PyTorch tensor\n",
    "        # observations_tensor = torch.tensor(observations, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "        observations_tensor = torch.tensor(observations, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        observations_tensor = observations_tensor.to(device)\n",
    "        actions = model(observations_tensor).detach().cpu().numpy()[0]\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        for i in range(p.getNumJoints(r2d2)):\n",
    "           \n",
    "            p.setJointMotorControl2(bodyIndex=1,jointIndex=i, controlMode=p.VELOCITY_CONTROL, targetVelocity=max_velocity_limit*actions[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        # next Observations\n",
    "        position, orientation = p.getBasePositionAndOrientation(r2d2)\n",
    "        x, y, z = position\n",
    "        roll, pitch, yaw = p.getEulerFromQuaternion(orientation)\n",
    "        joint_states = p.getJointStates(r2d2, range(p.getNumJoints(r2d2)))\n",
    "        joint_positions = [state[0] for state in joint_states]  # Joint positions\n",
    "        joint_velocities = [state[1] for state in joint_states]  # Joint velocities\n",
    "        contact_points = len(p.getContactPoints(bodyA=ground, bodyB=r2d2))\n",
    "        linear_vel, angular_vel = p.getBaseVelocity(r2d2)\n",
    "        joint_torques = [state[3] for state in joint_states]  # Joint torques\n",
    "        \n",
    "        # Combine all observations\n",
    "        next_observations = [x, y, z, roll, pitch, yaw] + joint_positions + joint_velocities + [contact_points] + list(linear_vel) +list(angular_vel) +list(joint_torques)\n",
    "    \n",
    "        # # Convert observations to a PyTorch tensor\n",
    "        # next_observations_tensor = torch.tensor(next_observations, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        reward = calculate_reward(position, orientation, joint_states, linear_vel, angular_vel, initial_position, initial_orientation,contact_points)\n",
    "    \n",
    "        \n",
    "        # Store data for training\n",
    "        if train:\n",
    "            # Convert observations and next_observations to PyTorch tensors\n",
    "            observations_tensor = torch.tensor(observations, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            next_observations_tensor = torch.tensor(next_observations, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            actions_tensor = torch.tensor(actions, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            reward_tensor = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "        \n",
    "            # Store the transition in replay memory\n",
    "        \n",
    "            # Calculate initial priority for the new experience\n",
    "            # Using absolute reward as a simple priority measure\n",
    "            initial_priority = abs(reward)\n",
    "    \n",
    "            # Store the transition with priority in replay memory\n",
    "            replay_memory.push(observations_tensor, actions_tensor, next_observations_tensor, reward_tensor, initial_priority)\n",
    "    \n",
    "    \n",
    "        # Perform training \n",
    "    \n",
    "        if train and len(replay_memory) >= batch_size and episode%50==0 :\n",
    "            episode+=1\n",
    "            trains+=1\n",
    "            train_alternator=train_alternator*-1\n",
    "            print('training',train_alternator)\n",
    "            gamma = 0.99\n",
    "            sequences = replay_memory.sample(batch_size)\n",
    "            for sequence in sequences:\n",
    "    \n",
    "                \n",
    "                batch = Transition(*zip(*sequence))\n",
    "            \n",
    "                # Concatenate the batch elements into separate tensors\n",
    "                state_batch = torch.cat(batch.state).unsqueeze(1)\n",
    "                action_batch = torch.cat(batch.action)\n",
    "                reward_batch = torch.cat(batch.reward)\n",
    "                next_state_batch = torch.cat(batch.next_state).unsqueeze(1)\n",
    "            \n",
    "                # Predict current Q-values\n",
    "                # print(state_batch.shape,reward_batch.shape)\n",
    "                current_q_values = model(state_batch)\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "                # Predict next Q-values\n",
    "                next_q_values = model(next_state_batch)\n",
    "            \n",
    "                # Average next Q-values across joints\n",
    "                average_next_q_values = torch.mean(next_q_values, dim=1)\n",
    "            \n",
    "                # Calculate TD target\n",
    "                td_target = reward_batch + gamma * average_next_q_values\n",
    "            \n",
    "                # Compute loss\n",
    "                # print(current_q_values)\n",
    "                loss = F.mse_loss(current_q_values, td_target.unsqueeze(1))\n",
    "        \n",
    "            \n",
    "                # Perform optimization step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "        p.stepSimulation()\n",
    "    \n",
    "            \n",
    "\n",
    "    # Evaluate the model\n",
    "    performance = reward\n",
    "\n",
    "    # If this is the best model so far, share it with other processes\n",
    "    with model_lock:\n",
    "        if performance > shared_performance.value:\n",
    "            shared_performance.value = performance\n",
    "            model_queue.put(model)\n",
    "            best_model_event.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f992aa-da6f-46a9-882d-eb27007facf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue, Event, Value, Lock\n",
    "import multiprocessing\n",
    "\n",
    "# Shared components\n",
    "model_queue = Queue()\n",
    "best_model_event = Event()\n",
    "shared_performance = Value('d', -float('inf'))  # Shared variable for best performance\n",
    "model_lock = Lock()  # Lock for updating shared performance\n",
    "\n",
    "# Number of robots\n",
    "num_robots = 4\n",
    "\n",
    "processes = []\n",
    "num_robots=1\n",
    "for i in range(num_robots):\n",
    "    q = Process(target=train_robot, args=(i, model_queue, best_model_event))\n",
    "    q.start()\n",
    "    processes.append(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e57cd-7235-43b3-8c62-19681d608e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in processes:\n",
    "    p.join()\n",
    "    print(f\"Process {p.name} alive: {p.is_alive()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1b7df-4983-4072-839e-bdee0bc0f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_queue.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487c234-cd32-4def-9665-f4f099f3fe63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
